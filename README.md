# Globant Data Engineering Challenge

## ğŸ“‹ Description

FastAPI application to process employee data from CSV files stored in S3 and store them in a database. The application is designed to be deployed on AWS ECS with Fargate.

## ğŸ—ï¸ Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    End User                        â”‚
â”‚              (consumes API endpoints)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                 â–²
                 â–¼                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Application Load Balancer (ALB)            â”‚
â”‚   (exposes public API endpoints)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             ECS Cluster (Fargate)                  â”‚
â”‚     (runs FastAPI container)                       â”‚
â”‚      - Task Definition (Docker image)              â”‚
â”‚      - Auto scaling                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                      â”‚
      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3 Bucket    â”‚    â”‚      Aurora Database       â”‚
â”‚  (CSV files)   â”‚    â”‚  (stores loaded data)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

## ğŸš€ Current Project Status

### âœ… Implemented

- **FastAPI Application**: REST API with endpoints for departments, jobs and employees
- **Data Models**: SQLModel for departments, jobs and employees
- **S3 Connection**: Function to read files from S3 buckets
- **Docker**: Container with FastAPI and hot-reload
- **Docker Compose**: Configuration for local development
- **Makefile**: Simplified commands for development and deployment
- **Logging**: Detailed logging system for debugging
- **Error Handling**: Robust error handling for AWS and S3

### ğŸ”§ Available Endpoints

- `GET /` - Root endpoint
- `GET /test-s3` - Test S3 connection and file reading
- `POST /departments` - Create department
- `GET /departments` - List departments
- `POST /jobs` - Create job
- `POST /employees` - Create employee

### ğŸ“ Project Structure

```
globant-data-engineering-challenge/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py          # FastAPI application
â”‚   â”œâ”€â”€ models.py        # SQLModel models
â”‚   â”œâ”€â”€ db.py           # Database configuration
â”‚   â””â”€â”€ s3_utils.py     # S3 utilities
â”œâ”€â”€ docker-compose.yml   # Docker Compose configuration
â”œâ”€â”€ Dockerfile          # Docker image definition
â”œâ”€â”€ Makefile           # Development commands
â”œâ”€â”€ requirements.txt   # Python dependencies
â””â”€â”€ README.md         # This file
```

## ğŸ› ï¸ Technologies Used

- **FastAPI**: Web framework for APIs
- **SQLModel**: Modern ORM for Python
- **boto3**: AWS SDK for Python
- **Docker**: Containers
- **Docker Compose**: Container orchestration
- **AWS S3**: File storage
- **AWS ECR**: Container registry (ready)

## ğŸš€ Installation and Usage

### Prerequisites

- Docker and Docker Compose
- AWS CLI configured with credentials
- Python 3.12+ (for local development)

### Local Development

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd globant-data-engineering-challenge
   ```

2. **Configure AWS credentials**
   ```bash
   aws configure
   ```

3. **Run the application**
   ```bash
   make run
   ```

4. **Test endpoints**
   ```bash
   make test
   make test-root
   ```

### Makefile Commands

```bash
make help      # View all commands
make build     # Build Docker image
make run       # Run application
make run-dev   # Run with visible logs
make stop      # Stop application
make logs      # View logs
make test      # Test S3 endpoint
make clean     # Clean containers
```

### Environment Variables

The application uses AWS CLI credentials automatically. If you need to use environment variables:

```bash
export AWS_ACCESS_KEY_ID=your_access_key
export AWS_SECRET_ACCESS_KEY=your_secret_key
export AWS_DEFAULT_REGION=us-east-1
```

## ğŸ” Testing

### Test S3 connection
```bash
curl "http://localhost:8000/test-s3?bucket_name=your-bucket"
```

### Test root endpoint
```bash
curl "http://localhost:8000/"
```

## ğŸ“ Next Steps

- [ ] Implement CSV file processing
- [ ] Connect to Aurora database
- [ ] Implement data validation
- [ ] Add unit tests
- [ ] Configure CI/CD pipeline
- [ ] Deploy to AWS ECS

## ğŸ¤ Contributing

1. Fork the project
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is under the MIT License - see the [LICENSE](LICENSE) file for details.

